input {
  # Syslog input for Docker logs (RFC5424 format)
  tcp {
    port => 5000
    codec => plain
  }
}

filter {
  # Step 1: Parse RFC5424 syslog format from Docker
  grok {
    match => { 
      "message" => "<%{NONNEGINT:syslog_pri}>%{NONNEGINT:syslog_ver} +(?:%{TIMESTAMP_ISO8601:syslog_timestamp}|-) +(?:%{IPORHOST:syslog_host}|-) +(?:%{NOTSPACE:service_tag}|-) +(?:%{POSINT:syslog_pid}|-) +(?:%{NOTSPACE:syslog_msgid}|-) +(?:%{NOTSPACE:syslog_sd}|-)?%{SPACE}?%{GREEDYDATA:log_message}"
    }
    tag_on_failure => ["_grokparsefailure_syslog"]
  }
  
  # Step 2: Try to parse JSON from the extracted log message
  if [log_message] {
    # Store original message for debugging
    mutate {
      copy => { "log_message" => "raw_message" }
    }
    
    # Try to parse as JSON
    json {
      source => "log_message"
      skip_on_invalid_json => true
      remove_field => ["log_message"]
    }
    
    # If JSON parsing failed and we still have log_message, it's plain text
    if [log_message] {
      mutate {
        rename => { "log_message" => "message" }
      }
    }
  }
  
  # Step 3: Handle Pino logger fields (from Fastify backend)
  if [level] {
    # Convert Pino log levels to standard names
    if [level] == 10 {
      mutate { add_field => { "log_level" => "TRACE" } }
    } else if [level] == 20 {
      mutate { add_field => { "log_level" => "DEBUG" } }
    } else if [level] == 30 {
      mutate { add_field => { "log_level" => "INFO" } }
    } else if [level] == 40 {
      mutate { add_field => { "log_level" => "WARN" } }
    } else if [level] == 50 {
      mutate { add_field => { "log_level" => "ERROR" } }
    } else if [level] == 60 {
      mutate { add_field => { "log_level" => "FATAL" } }
    }
  }
  
  # Step 4: Extract HTTP request information from Pino/Fastify logs
  if [req] {
    mutate {
      add_field => {
        "http.method" => "%{[req][method]}"
        "http.url" => "%{[req][url]}"
        "http.version" => "%{[req][httpVersion]}"
      }
    }
    
    # Extract request ID if present
    if [req][id] {
      mutate {
        add_field => { "request_id" => "%{[req][id]}" }
      }
    }
    
    # Extract remoteAddress (client IP)
    if [req][remoteAddress] {
      mutate {
        add_field => { "client.ip" => "%{[req][remoteAddress]}" }
      }
    }
  }
  
  # Step 5: Extract HTTP response information
  if [res] {
    if [res][statusCode] {
      mutate {
        add_field => { "http_status" => "%{[res][statusCode]}" }
      }
    }
  }
  
  # Step 6: Handle response time
  if [responseTime] {
    mutate {
      add_field => { "response_time_ms" => "%{[responseTime]}" }
    }
    mutate {
      convert => { "response_time_ms" => "float" }
    }
  }
  
  # Step 7: Extract service name and add metadata
  if [service_tag] {
    if [service_tag] == "frontend" {
      mutate {
        add_field => {
          "service.name" => "frontend"
          "service.type" => "web"
          "app.component" => "ui"
        }
        add_tag => [ "frontend", "web", "ui" ]
      }
    } else if [service_tag] == "backend" {
      mutate {
        add_field => {
          "service.name" => "backend"
          "service.type" => "api"
          "app.component" => "server"
        }
        add_tag => [ "backend", "api", "server" ]
      }
    } else {
      mutate {
        add_field => { "service.name" => "%{service_tag}" }
      }
    }
  }
  
  # Step 8: Extract log level from message if not already set
  if ![log_level] {
    grok {
      match => { "msg" => "(?i)\b(?<log_level>DEBUG|INFO|WARN|WARNING|ERROR|FATAL|TRACE)\b" }
      tag_on_failure => []
    }
  }
  
  # Normalize log level to uppercase
  if [log_level] {
    mutate {
      uppercase => [ "log_level" ]
    }
  } else {
    mutate {
      add_field => { "log_level" => "INFO" }
    }
  }
  
  # Step 9: Extract HTTP status codes if not already extracted
  if ![http_status] and [msg] {
    grok {
      match => { "msg" => "\b(?<http_status>[1-5][0-9]{2})\b" }
      tag_on_failure => []
    }
  }
  
  # Step 10: Categorize HTTP status codes
  if [http_status] {
    mutate {
      convert => { "http_status" => "integer" }
    }
    
    if [http_status] >= 500 {
      mutate { add_tag => ["error", "server_error"] }
    } else if [http_status] >= 400 {
      mutate { add_tag => ["error", "client_error"] }
    } else if [http_status] >= 300 {
      mutate { add_tag => ["redirect"] }
    } else if [http_status] >= 200 {
      mutate { add_tag => ["success"] }
    }
  }
  
  # Step 11: Use Pino timestamp or syslog timestamp
  if [time] {
    date {
      match => [ "time", "UNIX_MS", "ISO8601" ]
      target => "@timestamp"
      tag_on_failure => ["_dateparsefailure"]
    }
  } else if [syslog_timestamp] {
    date {
      match => [ "syslog_timestamp", "ISO8601" ]
      target => "@timestamp"
      tag_on_failure => ["_dateparsefailure"]
    }
  }
  
  # Step 12: Create a clean message field
  if [msg] and ![message] {
    mutate {
      copy => { "msg" => "message" }
    }
  }
  
  # Step 13: Add environment and metadata
  mutate {
    add_field => {
      "environment" => "development"
      "log.source" => "docker-syslog"
      "log.format" => "rfc5424"
    }
  }
  
  # Step 14: Clean up unnecessary/duplicate fields
  mutate {
    remove_field => [ 
      "syslog_pri", "syslog_ver", "syslog_msgid", "syslog_sd",
      "syslog_pid", "syslog_host", "syslog_timestamp",
      "level", "pid", "hostname", "v", "service_tag",
      "req", "res", "time", "msg"
    ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "ft_transcendence-%{+YYYY.MM.dd}"
    document_type => "_doc"
  }
  
  # Uncomment for debugging:
  # stdout { codec => rubydebug }
}